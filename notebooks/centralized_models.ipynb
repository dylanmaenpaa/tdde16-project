{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQg1BG-Rlpo7"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyjymBKWAJ4F"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq6MOUgYl1pX"
      },
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZoh2GzFcQM7"
      },
      "source": [
        "import pandas as pd\r\n",
        "import bz2\r\n",
        "\r\n",
        "with bz2.open('/content/drive/MyDrive/Universitet/Thesis/NLP code/speeches-201718.json.bz2') as source:\r\n",
        "    speeches_201718 = pd.read_json(source)\r\n",
        "\r\n",
        "with bz2.open('/content/drive/MyDrive/Universitet/Thesis/NLP code/speeches-201819.json.bz2') as source:\r\n",
        "    speeches_201819 = pd.read_json(source)\r\n",
        "\r\n",
        "speeches = pd.concat([speeches_201718, speeches_201819])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHo8SY22qy68"
      },
      "source": [
        "### Preprocess data\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSMlLwJ7q0K4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "party_to_int = {\r\n",
        "    'S': 0,\r\n",
        "    'M': 1,\r\n",
        "    'MP': 2,\r\n",
        "    'SD': 3,\r\n",
        "    'V': 4,\r\n",
        "    'C': 5,\r\n",
        "    'KD': 6,\r\n",
        "    'L': 7,\r\n",
        "}\r\n",
        "\r\n",
        "def speeches_split(speeches, max_text_length=1000):\r\n",
        "  texts = []\r\n",
        "  labels = []\r\n",
        "  for _, row in speeches.iterrows():\r\n",
        "    words_arr = row[\"words\"].split(\" \")\r\n",
        "    words_arr = words_arr[-max_text_length:]\r\n",
        "    words = ' '.join(map(str, words_arr)) \r\n",
        "    texts.append(words)\r\n",
        "    party = row[\"party\"]\r\n",
        "    party_int = party_to_int[party]\r\n",
        "    labels.append(party_int)\r\n",
        "  return texts, labels\r\n",
        "\r\n",
        "train_texts, train_labels = speeches_split(speeches)\r\n",
        "\r\n",
        "# Split data set into 80/20\r\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=.2)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJVBPG8NmjiO"
      },
      "source": [
        "## Import pre-trained Swedish bert and tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKZHPZjB_ha0"
      },
      "source": [
        "from transformers import AutoModel,AutoTokenizer,TFAutoModel\r\n",
        "\r\n",
        "tokenizer = AutoTokenizer.from_pretrained('KB/bert-base-swedish-cased')\r\n",
        "model = AutoModel.from_pretrained('KB/bert-base-swedish-cased')\r\n",
        "\r\n",
        "train_encodings = tokenizer(train_texts, truncation=True, max_length=256, padding=True)\r\n",
        "test_encodings = tokenizer(test_texts, truncation=True, max_length=256, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AzwuHnRoDb8"
      },
      "source": [
        "## Plot data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW-SIR81WQLI"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Distribution of speeches over the parties\r\n",
        "fig, axes = plt.subplots(nrows=1,ncols=2)\r\n",
        "speeches['party'].value_counts().plot(ax=axes[0],subplots=True, kind='bar', color=[\"red\", \"blue\", \"limegreen\", \"yellow\", \"darkred\", \"seagreen\", \"darkblue\", \"lightskyblue\"], x=\"Party\", figsize=(5, 3))\r\n",
        "axes[0].title.set_text('Class distribution')\r\n",
        "axes[0].set_xlabel('Party')\r\n",
        "axes[0].set_ylabel('Number of speeches')\r\n",
        "\r\n",
        "# Token lengths for all speeches\r\n",
        "# for box plot\r\n",
        "len_tokens = []\r\n",
        "for speech in speeches[\"words\"]:\r\n",
        "  t = tokenizer(speech)\r\n",
        "  len_tokens.append(len(t[\"input_ids\"])-2)\r\n",
        "\r\n",
        "fig.subplots_adjust(wspace=0.5)\r\n",
        "fig.set_figheight(5)\r\n",
        "fig.set_figwidth(10)\r\n",
        "\r\n",
        "data = [len_tokens]\r\n",
        "axes[1].set_title('Statistical analysis')\r\n",
        "axes[1].boxplot(data, showfliers=False, labels=[\"Speeches\"])\r\n",
        "axes[1].set_ylabel(\"Number of tokens\")\r\n",
        "axes[1].set_xlabel(\"Data set\")\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSacxnU-oKas"
      },
      "source": [
        "# Save fig\r\n",
        "from google.colab import files\r\n",
        "fig.savefig(\"abc.png\")\r\n",
        "files.download(\"abc.png\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVtS3qXcnAXa"
      },
      "source": [
        "\r\n",
        "## Setup for pytorch\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q50NLvNaAdjh"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "class SpeechDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, encodings, labels):\r\n",
        "        self.encodings = encodings\r\n",
        "        self.labels = labels\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\r\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\r\n",
        "        return item\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.labels)\r\n",
        "\r\n",
        "train_dataset = SpeechDataset(train_encodings, train_labels)\r\n",
        "test_dataset = SpeechDataset(test_encodings, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew_H17crnMQL"
      },
      "source": [
        "## Adapt pre-trained BERT\r\n",
        "Add a dropout layer, and a linear layer on top of the pooled BERT output.\r\n",
        "Calculate cross entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qMHHQ3O0-fo"
      },
      "source": [
        "from torch import nn\r\n",
        "\r\n",
        "class CustomBERTModel(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(CustomBERTModel, self).__init__()\r\n",
        "        self.num_labels = 8\r\n",
        "\r\n",
        "        self.bert = AutoModel.from_pretrained('KB/bert-base-swedish-cased')\r\n",
        "        self.dropout = nn.Dropout(0.1)\r\n",
        "        self.classifier = nn.Linear(768, 8)\r\n",
        "\r\n",
        "    def forward(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        labels=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "    ):\r\n",
        "        r\"\"\"\r\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\r\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\r\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\r\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        outputs = self.bert(\r\n",
        "            input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "        )\r\n",
        "\r\n",
        "        pooled_output = outputs[1]\r\n",
        "        pooled_output = self.dropout(pooled_output)\r\n",
        "        logits = self.classifier(pooled_output)\r\n",
        "\r\n",
        "        loss_fct = nn.CrossEntropyLoss()\r\n",
        "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n",
        "        output = (logits,) + outputs[2:]\r\n",
        "        return ((loss,) + output) if loss is not None else output\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyeLe0bcnnvf"
      },
      "source": [
        "## Trainer setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V-CHiftAz8q"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\r\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "model = CustomBERTModel()\r\n",
        "model = model.cuda()\r\n",
        "class_reports = []\r\n",
        "\r\n",
        "def compute_metrics(pred):\r\n",
        "    labels = pred.label_ids\r\n",
        "    preds = pred.predictions.argmax(-1)\r\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\r\n",
        "    acc = accuracy_score(labels, preds)\r\n",
        "    cr = classification_report(labels, preds)\r\n",
        "    print(cr)\r\n",
        "    class_reports.append(cr)\r\n",
        "    return {\r\n",
        "        'accuracy': acc,\r\n",
        "        'f1': f1,\r\n",
        "        'precision': precision,\r\n",
        "        'recall': recall\r\n",
        "    }\r\n",
        "  \r\n",
        "epochs = 10\r\n",
        "batch_size = 8\r\n",
        "warm_steps = int(len(train_texts)*0.1/batch_size*epochs)\r\n",
        "\r\n",
        "training_args = TrainingArguments(\r\n",
        "    output_dir='./results',                      # output directory\r\n",
        "    num_train_epochs=epochs,                         # total number of training epochs\r\n",
        "    evaluation_strategy=\"epoch\",\r\n",
        "    learning_rate=5e-5,\r\n",
        "    per_device_train_batch_size=batch_size,      # batch size per device during training\r\n",
        "    per_device_eval_batch_size=batch_size*2,     # batch size for evaluation\r\n",
        "    warmup_steps=warm_steps,                     # number of warmup steps for learning rate scheduler\r\n",
        "    weight_decay=0.01,                           # strength of weight decay\r\n",
        "    logging_dir='./logs',                        # directory for storing logs\r\n",
        "    logging_steps=10,\r\n",
        "    save_steps=1082,\r\n",
        ")\r\n",
        "\r\n",
        "trainer = Trainer(\r\n",
        "    model=model,                          # the instantiated 🤗 Transformers model to be trained\r\n",
        "    args=training_args,                   # training arguments, defined above\r\n",
        "    train_dataset=train_dataset,          # training dataset\r\n",
        "    eval_dataset=test_dataset,             # evaluation dataset\r\n",
        "    compute_metrics=compute_metrics\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_DhXOR3ns2K"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhQpXVqgnhwq"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HibTCwgLBu4M"
      },
      "source": [
        "# Save model\r\n",
        "torch.save(model.state_dict(), './models.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTc9YVIGoT0s"
      },
      "source": [
        "## Check logs on tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RST_9PlXF_OE"
      },
      "source": [
        "# Load the TensorBoard notebook extension\r\n",
        "# Load the TensorBoard notebook extension\r\n",
        "!kill 528\r\n",
        "%reload_ext tensorboard\r\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldnlfKHctiLh"
      },
      "source": [
        "## Pickle save class reports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnCKq6TzsKqB"
      },
      "source": [
        "import pickle \r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Create array of random numbers:\r\n",
        "np_class_reports = np.array(class_reports)\r\n",
        "\r\n",
        "filename = 'pickled_class_reports.p'\r\n",
        "with open(filename, 'wb') as filehandler:\r\n",
        "    pickle.dump(np_class_reports, filehandler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIGCUKoktOKH"
      },
      "source": [
        "# Pickle read\r\n",
        "with open(filename, 'rb') as filehandler: \r\n",
        "    reloaded_array = pickle.load(filehandler)\r\n",
        "\r\n",
        "print ('Reloaded array:',reloaded_array)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}