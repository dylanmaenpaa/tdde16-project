{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fed_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQg1BG-Rlpo7"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyjymBKWAJ4F"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGESGuFSBgM1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq6MOUgYl1pX"
      },
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZoh2GzFcQM7"
      },
      "source": [
        "import pandas as pd\r\n",
        "import bz2\r\n",
        "\r\n",
        "with bz2.open('/content/drive/MyDrive/Universitet/Thesis/NLP code/speeches-201718.json.bz2') as source:\r\n",
        "    speeches_201718 = pd.read_json(source)\r\n",
        "\r\n",
        "with bz2.open('/content/drive/MyDrive/Universitet/Thesis/NLP code/speeches-201819.json.bz2') as source:\r\n",
        "    speeches_201819 = pd.read_json(source)\r\n",
        "\r\n",
        "speeches = pd.concat([speeches_201718, speeches_201819])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVYSVZDnzrnY"
      },
      "source": [
        "## Global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqiKEXvCV-Wt"
      },
      "source": [
        "# number of locations\r\n",
        "L = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHo8SY22qy68"
      },
      "source": [
        "### Preprocess data\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSMlLwJ7q0K4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "party_to_int = {\r\n",
        "    'S': 0,\r\n",
        "    'M': 1,\r\n",
        "    'MP': 2,\r\n",
        "    'SD': 3,\r\n",
        "    'V': 4,\r\n",
        "    'C': 5,\r\n",
        "    'KD': 6,\r\n",
        "    'L': 7,\r\n",
        "}\r\n",
        "\r\n",
        "def speeches_split(speeches, max_text_length=1000):\r\n",
        "  texts = []\r\n",
        "  labels = []\r\n",
        "  for _, row in speeches.iterrows():\r\n",
        "    words_arr = row[\"words\"].split(\" \")\r\n",
        "    words_arr = words_arr[-max_text_length:]\r\n",
        "    words = ' '.join(map(str, words_arr)) \r\n",
        "    texts.append(words)\r\n",
        "    party = row[\"party\"]\r\n",
        "    party_int = party_to_int[party]\r\n",
        "    labels.append(party_int)\r\n",
        "  return texts, labels\r\n",
        "\r\n",
        "train_texts, train_labels = speeches_split(speeches)\r\n",
        "\r\n",
        "# Split data set into 80/20\r\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=.2)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-Nuevi8_Oin"
      },
      "source": [
        "## Split data into n subsets\r\n",
        "\r\n",
        "def split_into_n_arrays(texts, labels, n):\r\n",
        "  texts_splitted = []\r\n",
        "  labels_splitted = []\r\n",
        "  texts_init_len = len(texts)\r\n",
        "  print(texts_init_len)\r\n",
        "  for i in range(n):\r\n",
        "    subset_text = []\r\n",
        "    subset_labels = []\r\n",
        "    if i != n-1:\r\n",
        "      for i in range(texts_init_len//n):\r\n",
        "        subset_text.append(texts.pop())\r\n",
        "        subset_labels.append(labels.pop())\r\n",
        "    else:\r\n",
        "      for i in range(len(texts)):\r\n",
        "        subset_text.append(texts.pop())\r\n",
        "        subset_labels.append(labels.pop())\r\n",
        "    texts_splitted.append(subset_text)\r\n",
        "    labels_splitted.append(subset_labels)\r\n",
        "  return texts_splitted, labels_splitted\r\n",
        "\r\n",
        "train_split, labels_split = split_into_n_arrays(train_texts, train_labels, L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJVBPG8NmjiO"
      },
      "source": [
        "## Import pre-trained Swedish bert and tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKZHPZjB_ha0"
      },
      "source": [
        "from transformers import AutoModel,AutoTokenizer,TFAutoModel\r\n",
        "import torch\r\n",
        "\r\n",
        "tokenizer = AutoTokenizer.from_pretrained('KB/bert-base-swedish-cased')\r\n",
        "\r\n",
        "def encode_arrays(train_split):\r\n",
        "  encoded_split = []\r\n",
        "  for texts in train_split:\r\n",
        "    encoded_split.append(tokenizer(texts, truncation=True, max_length=256, padding=True))\r\n",
        "  return encoded_split\r\n",
        "\r\n",
        "test_encodings = tokenizer(test_texts, truncation=True, max_length=256, padding=True)\r\n",
        "train_L_encodings = encode_arrays(train_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVtS3qXcnAXa"
      },
      "source": [
        "\r\n",
        "## Setup for pytorch\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q50NLvNaAdjh"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "class SpeechDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, encodings, labels):\r\n",
        "        self.encodings = encodings\r\n",
        "        self.labels = labels\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\r\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\r\n",
        "        return item\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.labels)\r\n",
        "\r\n",
        "def gen_train_datasets(train_multiple_encodings, labels_split):\r\n",
        "  train_datasets = []\r\n",
        "  for i, train_encs in enumerate(train_multiple_encodings):\r\n",
        "    labels = labels_split[i]\r\n",
        "    train_dataset = SpeechDataset(train_encs, labels)\r\n",
        "    train_datasets.append(train_dataset)\r\n",
        "  return train_datasets\r\n",
        "\r\n",
        "#train_dataset = SpeechDataset(train_encodings, train_labels)\r\n",
        "train_datasets = gen_train_datasets(train_L_encodings, labels_split)\r\n",
        "test_dataset = SpeechDataset(test_encodings, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew_H17crnMQL"
      },
      "source": [
        "## Adapt pre-trained BERT\r\n",
        "Add a dropout layer, and a linear layer on top of the pooled BERT output.\r\n",
        "Calculate cross entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qMHHQ3O0-fo"
      },
      "source": [
        "from torch import nn\r\n",
        "\r\n",
        "class CustomBERTModel(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(CustomBERTModel, self).__init__()\r\n",
        "        self.num_labels = 8\r\n",
        "\r\n",
        "        self.bert = AutoModel.from_pretrained('KB/bert-base-swedish-cased')\r\n",
        "        self.dropout = nn.Dropout(0.1)\r\n",
        "        self.classifier = nn.Linear(768, 8)\r\n",
        "\r\n",
        "    def forward(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        labels=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "    ):\r\n",
        "        r\"\"\"\r\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\r\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\r\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\r\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        outputs = self.bert(\r\n",
        "            input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "        )\r\n",
        "\r\n",
        "        pooled_output = outputs[1]\r\n",
        "        pooled_output = self.dropout(pooled_output)\r\n",
        "        logits = self.classifier(pooled_output)\r\n",
        "\r\n",
        "        loss_fct = nn.CrossEntropyLoss()\r\n",
        "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n",
        "        output = (logits,) + outputs[2:]\r\n",
        "        return ((loss,) + output) if loss is not None else output\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyeLe0bcnnvf"
      },
      "source": [
        "## Training setup and fedavg code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLDY96B9xgYa"
      },
      "source": [
        "from torch.utils.data import DataLoader\r\n",
        "from transformers import AdamW\r\n",
        "import transformers\r\n",
        "import math\r\n",
        "import copy\r\n",
        "\r\n",
        "## TRAINING GLOBAL VARIABLES ##\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "batch_size = 4\r\n",
        "cycles = 10\r\n",
        "\r\n",
        "def fedavg(models):\r\n",
        "  params1 = models[0]['model'].named_parameters()\r\n",
        "  resulting_params = models[0]['model'].named_parameters()\r\n",
        "  resulting_params = copy.deepcopy(dict(resulting_params))\r\n",
        "\r\n",
        "  # N = total number of training samples\r\n",
        "  N = 17304\r\n",
        "\r\n",
        "  for name, _ in params1:\r\n",
        "    total = 0\r\n",
        "    for model_dict in models:\r\n",
        "      model = model_dict['model']\r\n",
        "      total += dict(model.named_parameters())[name]*model_dict['n_samples']/N\r\n",
        "    #if not torch.all(torch.eq(total, resulting_params[name])):\r\n",
        "      #print(\"\")\r\n",
        "    resulting_params[name].data.copy_(total)\r\n",
        "    \r\n",
        "  model = CustomBERTModel()\r\n",
        "  model = model.cuda()\r\n",
        "  model.to(device)\r\n",
        "  model.load_state_dict(resulting_params, strict=False)\r\n",
        "  return model\r\n",
        "\r\n",
        "def create_n_models(n, train_datasets):\r\n",
        "  models = []\r\n",
        "  for i in range(n):\r\n",
        "    model = CustomBERTModel()\r\n",
        "    model = model.cuda()\r\n",
        "    model.to(device)\r\n",
        "    train_loader = DataLoader(train_datasets[i], batch_size=batch_size, shuffle=True)\r\n",
        "    optim = AdamW(model.parameters(), lr=5e-5,  weight_decay=0.01)\r\n",
        "    warm_steps = int(len(train_datasets[i])*0.1/batch_size*cycles)\r\n",
        "    train_steps = int(len(train_datasets[i])/batch_size*cycles)\r\n",
        "    scheduler = transformers.get_linear_schedule_with_warmup(optim, warm_steps, train_steps)\r\n",
        "    models.append({'model': model, 'loader': train_loader, 'optim': optim, 'scheduler': scheduler, 'n_samples': len(train_datasets[i])})\r\n",
        "  return models\r\n",
        "\r\n",
        "\r\n",
        "def update_models(global_model, models):\r\n",
        "  for i in range(L):\r\n",
        "    # load global model parameters\r\n",
        "    m = models[i]['model']\r\n",
        "    m.load_state_dict(copy.deepcopy(dict(global_model.named_parameters())), strict=False)\r\n",
        "    models[i]['model'] = m\r\n",
        "  return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHYfdGPM0YAA"
      },
      "source": [
        "### Evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHPMhhZZXbol"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\r\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "def compute_metrics(pred):\r\n",
        "    labels = pred['label_ids']\r\n",
        "    preds = pred['predictions'].argmax(-1)\r\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\r\n",
        "    acc = accuracy_score(labels, preds)\r\n",
        "    cr = classification_report(labels, preds, digits=3)\r\n",
        "    print(cr)\r\n",
        "    return {\r\n",
        "        'accuracy': acc,\r\n",
        "        'f1': f1,\r\n",
        "        'precision': precision,\r\n",
        "        'recall': recall\r\n",
        "    }\r\n",
        "\r\n",
        "def evaluate(model):\r\n",
        "  test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\r\n",
        "  pred = {\r\n",
        "          'label_ids': torch.empty(0).to(device),\r\n",
        "          'predictions':torch.empty(0).to(device)\r\n",
        "          }\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  with torch.no_grad():\r\n",
        "    for i, batch in enumerate(test_loader):\r\n",
        "      #print(\"batch: {}/{}\".format(i, len(test_loader)))\r\n",
        "      input_ids = batch['input_ids'].to(device)\r\n",
        "      attention_mask = batch['attention_mask'].to(device)\r\n",
        "      labels = batch['labels'].to(device)\r\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\r\n",
        "      pred['predictions'] = torch.cat((pred['predictions'], outputs[1]), 0)\r\n",
        "      pred['label_ids'] = torch.cat((pred['label_ids'], labels), 0)\r\n",
        "\r\n",
        "  pred['predictions'] = pred['predictions'].cpu()\r\n",
        "  pred['label_ids'] = pred['label_ids'].cpu()\r\n",
        "\r\n",
        "  compute_metrics(pred)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_DhXOR3ns2K"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWITFsQsHqdT"
      },
      "source": [
        "def same(m1, m2):\r\n",
        "  for p1, p2 in zip(m1.parameters(), m2.parameters()):\r\n",
        "      if p1.data.ne(p2.data).sum() > 0:\r\n",
        "          print(p1.data.ne(p2.data).sum())\r\n",
        "          return False\r\n",
        "  return True\r\n",
        "\r\n",
        "def same2(m1, m2):\r\n",
        "  m2_dict = dict(m2.named_parameters())\r\n",
        "  for n, p in m1.named_parameters():\r\n",
        "    if not torch.all(torch.eq(p, m2_dict[n])):\r\n",
        "      print(n)\r\n",
        "      print(p)\r\n",
        "      print(m2_dict[n])\r\n",
        "      return False\r\n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyODB7LFKsID"
      },
      "source": [
        "import time\r\n",
        "start_time = time.time()\r\n",
        "\r\n",
        "def train():\r\n",
        "  models = create_n_models(L, train_datasets)\r\n",
        "  for c in range(cycles):\r\n",
        "    print(\"Cycle: {}\".format(c+1))\r\n",
        "    for j, model_dict in enumerate(models):\r\n",
        "      print(\"Training model {}\".format(j))\r\n",
        "      model = model_dict['model']\r\n",
        "      test = copy.deepcopy(model_dict['model'])\r\n",
        "      model.train()\r\n",
        "      train_loader = model_dict['loader']\r\n",
        "      optim = model_dict['optim']\r\n",
        "      scheduler = model_dict['scheduler']\r\n",
        "      #for epoch in range(epochs):\r\n",
        "      for i, batch in enumerate(train_loader):\r\n",
        "        #print(\"\\rbatch: {}/{}\".format(i, len(train_loader), end=\"\"))\r\n",
        "        optim.zero_grad()\r\n",
        "        input_ids = batch['input_ids'].to(device)\r\n",
        "        attention_mask = batch['attention_mask'].to(device)\r\n",
        "        labels = batch['labels'].to(device)\r\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\r\n",
        "        loss = outputs[0]\r\n",
        "        loss.backward()\r\n",
        "        optim.step()\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "    print(\"learning rates.........\")\r\n",
        "    for mdl in models:\r\n",
        "      for param_group in mdl['optim'].param_groups:\r\n",
        "        print(param_group['lr'])\r\n",
        "\r\n",
        "\r\n",
        "    print()\r\n",
        "    \r\n",
        "    for i, m in enumerate(models):\r\n",
        "      print(\"Evaluating model {} ------------------...\".format(i))\r\n",
        "      evaluate(m['model'])\r\n",
        "    print(\"Evaluating global model...\")\r\n",
        "    global_model = fedavg(models)\r\n",
        "    evaluate(global_model)\r\n",
        "    models = update_models(global_model, models)\r\n",
        "    del global_model\r\n",
        "  return models\r\n",
        "\r\n",
        "mdls = train()\r\n",
        "\r\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldnlfKHctiLh"
      },
      "source": [
        "## Save models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJJWu2et8AFD"
      },
      "source": [
        "g_m = fedavg(mdls)\r\n",
        "torch.save(g_m.state_dict(), '/content/drive/MyDrive/5_run2_models.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5ugIv3byS5t"
      },
      "source": [
        "## Evaluate saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I7pLSmqqMXx"
      },
      "source": [
        "model = CustomBERTModel()\r\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/5_run_2models.pth'))\r\n",
        "model.cuda()\r\n",
        "evaluate(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0MZBfVqXMSF"
      },
      "source": [
        "## Plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDyQojHIXR3f"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "precision = [0.651, 0.638, 0.640, 0.630, 0.605]\r\n",
        "recall = [0.608, 0.613, 0.612, 0.602, 0.582]\r\n",
        "f_one = [0.625, 0.624, 0.623, 0.614, 0.591]\r\n",
        "accuracy = [0.676, 0.678, 0.685, 0.673, 0.648]\r\n",
        "\r\n",
        "y = accuracy \r\n",
        "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\r\n",
        "font = {'family' : 'normal',\r\n",
        "        'size'   : 14}\r\n",
        "\r\n",
        "matplotlib.rc('font', **font)\r\n",
        "matplotlib.rc('font', **font)\r\n",
        "\r\n",
        "x = np.arange(1, 6, 1)\r\n",
        "axs[0].set_xticks(x)\r\n",
        "#fig.suptitle('Experimental results', fontsize=16)\r\n",
        "#plt.subplots_adjust(wspace=2)\r\n",
        "axs[0].plot(x, f_one)\r\n",
        "axs[0].set_title('Results')\r\n",
        "axs[0].set(xlabel='Number of locations', ylabel='Value')\r\n",
        "axs[0].plot(x, precision, 'tab:orange')\r\n",
        "axs[0].plot(x, recall, 'tab:green')\r\n",
        "axs[0].plot(x, accuracy, 'tab:red')\r\n",
        "axs[0].set_ylim((0.57, 0.70))\r\n",
        "axs[0].legend(['F1-Score', 'Precision', 'Recall', 'Accuracy'], loc='upper right')\r\n",
        "columns = ('1', '2', '3', '4.', '5')\r\n",
        "rows = ['F1-score', 'Precision', 'Recall', 'Accuracy']\r\n",
        "\r\n",
        "data_list = [f_one, precision, recall, accuracy]\r\n",
        "scatter_x = (1, 2, 3)\r\n",
        "scatter_y = (1224.53, 1231.76, 1228.70)\r\n",
        "\r\n",
        "\r\n",
        "table = axs[1].table(cellText=data_list,\r\n",
        "           rowLabels=rows,\r\n",
        "           colLabels=columns, loc=\"center\",\r\n",
        "           colWidths=[0.1 for _ in range(5)],\r\n",
        "           fontsize=20\r\n",
        "            )\r\n",
        "            \r\n",
        "table.set_fontsize(14)\r\n",
        "table.scale(1.5, 1.5)\r\n",
        "\r\n",
        "axs[1].axis(\"off\")\r\n",
        "\r\n",
        "fig.savefig(\"res.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXGJU7S-Xdin"
      },
      "source": [
        "# Google colab download\r\n",
        "from google.colab import files\r\n",
        "files.download(\"res.png\") "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}